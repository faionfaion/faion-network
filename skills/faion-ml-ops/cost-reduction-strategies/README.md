---
id: cost-reduction-strategies
name: "Cost Reduction Strategies"
domain: ML
skill: faion-ml-engineer
category: "ml-engineering"
---

# Cost Reduction Strategies

Advanced techniques to reduce LLM API costs while maintaining quality: caching, prompt optimization, batching, and model routing.

## When to Use

- High-volume production apps
- Budget-constrained projects
- Scaling deployments
- Optimizing existing apps

## Response Caching

```python
import hashlib
import json
from typing import Optional, Dict
from datetime import datetime, timedelta
import redis

class ResponseCache:
    """Cache LLM responses to avoid redundant calls."""

    def __init__(self, redis_client: redis.Redis = None, ttl_hours: int = 24, max_memory_items: int = 1000):
        self.redis = redis_client
        self.ttl = timedelta(hours=ttl_hours)
        self.memory_cache: Dict[str, Dict] = {}
        self.max_memory_items = max_memory_items

    def _make_key(self, model: str, messages: list, **kwargs) -> str:
        """Create cache key from request parameters."""
        key_data = {
            "model": model,
            "messages": messages,
            "temperature": kwargs.get("temperature", 1.0),
            "max_tokens": kwargs.get("max_tokens"),
        }
        return hashlib.sha256(json.dumps(key_data, sort_keys=True).encode()).hexdigest()

    def get(self, model: str, messages: list, **kwargs) -> Optional[str]:
        """Get cached response if available."""
        key = self._make_key(model, messages, **kwargs)

        if key in self.memory_cache:
            entry = self.memory_cache[key]
            if datetime.now() < entry["expires"]:
                return entry["response"]
            else:
                del self.memory_cache[key]

        if self.redis:
            cached = self.redis.get(f"llm_cache:{key}")
            if cached:
                return json.loads(cached)

        return None

    def set(self, model: str, messages: list, response: str, **kwargs):
        """Cache a response."""
        key = self._make_key(model, messages, **kwargs)

        if len(self.memory_cache) >= self.max_memory_items:
            oldest_keys = sorted(self.memory_cache.keys(), key=lambda k: self.memory_cache[k]["created"])[:100]
            for k in oldest_keys:
                del self.memory_cache[k]

        self.memory_cache[key] = {
            "response": response,
            "created": datetime.now(),
            "expires": datetime.now() + self.ttl
        }

        if self.redis:
            self.redis.setex(f"llm_cache:{key}", int(self.ttl.total_seconds()), json.dumps(response))

class CachedLLMClient:
    """LLM client with caching."""

    def __init__(self, client, cache: ResponseCache = None):
        self.client = client
        self.cache = cache or ResponseCache()
        self.cache_hits = 0
        self.cache_misses = 0

    def complete(self, model: str, messages: list, use_cache: bool = True, **kwargs) -> str:
        """Get completion with caching."""
        temperature = kwargs.get("temperature", 1.0)
        cacheable = use_cache and temperature == 0

        if cacheable:
            cached = self.cache.get(model, messages, **kwargs)
            if cached:
                self.cache_hits += 1
                return cached

        self.cache_misses += 1

        response = self.client.chat.completions.create(model=model, messages=messages, **kwargs)
        result = response.choices[0].message.content

        if cacheable:
            self.cache.set(model, messages, result, **kwargs)

        return result
```

## Prompt Optimization

```python
import tiktoken
import re

class PromptOptimizer:
    """Optimize prompts for cost efficiency."""

    def __init__(self, model: str = "gpt-4o"):
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.encoding.encode(text))

    def truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """Truncate text to max tokens."""
        tokens = self.encoding.encode(text)
        if len(tokens) <= max_tokens:
            return text
        return self.encoding.decode(tokens[:max_tokens])

    def compress_prompt(self, prompt: str) -> str:
        """Apply basic compression techniques."""
        compressed = re.sub(r'\s+', ' ', prompt)

        redundant = ["please ", "kindly ", "could you please ", "I would like you to ", "I need you to "]
        for phrase in redundant:
            compressed = compressed.replace(phrase, "")

        return compressed.strip()

    def optimize_system_prompt(self, system_prompt: str) -> str:
        """Optimize system prompt for brevity."""
        verbose_to_concise = {
            "You are a helpful assistant that": "You are an assistant that",
            "Please make sure to": "",
            "It is important that you": "",
            "Remember to always": "",
        }

        optimized = system_prompt
        for verbose, concise in verbose_to_concise.items():
            optimized = optimized.replace(verbose, concise)

        return optimized.strip()

    def summarize_context(self, context: str, max_tokens: int, client) -> str:
        """Summarize long context to reduce tokens."""
        current_tokens = self.count_tokens(context)

        if current_tokens <= max_tokens:
            return context

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"Summarize the following text in approximately {max_tokens} tokens, preserving key information."
                },
                {"role": "user", "content": context}
            ],
            max_tokens=max_tokens
        )

        return response.choices[0].message.content
```

## Batching Requests

```python
import asyncio
from typing import List
from openai import AsyncOpenAI

class BatchProcessor:
    """Process multiple requests efficiently."""

    def __init__(self, client: AsyncOpenAI, batch_size: int = 10, delay_between_batches: float = 0.1):
        self.client = client
        self.batch_size = batch_size
        self.delay = delay_between_batches

    async def process_batch(self, prompts: List[str], model: str = "gpt-4o-mini", **kwargs) -> List[str]:
        """Process multiple prompts in parallel batches."""
        results = []

        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i:i + self.batch_size]
            tasks = [self._single_request(prompt, model, **kwargs) for prompt in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(batch_results)

            if i + self.batch_size < len(prompts):
                await asyncio.sleep(self.delay)

        return results

    async def _single_request(self, prompt: str, model: str, **kwargs) -> str:
        """Make a single async request."""
        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error: {e}"
```

## Production Cost Optimization Service

```python
from dataclasses import dataclass
from typing import Optional, Dict, Any
import logging

@dataclass
class CostOptimizationConfig:
    enable_caching: bool = True
    enable_model_routing: bool = True
    enable_prompt_compression: bool = True
    cache_ttl_hours: int = 24
    default_model: str = "gpt-4o-mini"
    fallback_model: str = "gpt-4o"
    max_retries: int = 3
    budget_limit_per_day: Optional[float] = None

class CostOptimizedLLM:
    """Production LLM service with cost optimization."""

    def __init__(self, client, config: Optional[CostOptimizationConfig] = None):
        self.client = client
        self.config = config or CostOptimizationConfig()
        self.logger = logging.getLogger(__name__)

        self.cache = ResponseCache(ttl_hours=self.config.cache_ttl_hours)
        self.optimizer = PromptOptimizer()

    def complete(self, prompt: str, system_prompt: str = "", force_model: Optional[str] = None,
                 use_cache: bool = True, **kwargs) -> Dict[str, Any]:
        """Get completion with all optimizations."""
        if self._is_over_budget():
            raise Exception("Daily budget limit exceeded")

        if self.config.enable_prompt_compression:
            prompt = self.optimizer.compress_prompt(prompt)
            system_prompt = self.optimizer.optimize_system_prompt(system_prompt)

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        model = force_model or self.config.default_model

        if self.config.enable_caching and use_cache:
            cached = self.cache.get(model, messages, **kwargs)
            if cached:
                self.logger.info(f"Cache hit for model {model}")
                return {"content": cached, "model": model, "from_cache": True, "cost": 0.0}

        try:
            response = self.client.chat.completions.create(model=model, messages=messages, **kwargs)
        except Exception as e:
            self.logger.warning(f"Model {model} failed, trying fallback")
            response = self.client.chat.completions.create(
                model=self.config.fallback_model,
                messages=messages,
                **kwargs
            )
            model = self.config.fallback_model

        content = response.choices[0].message.content

        if self.config.enable_caching and use_cache:
            temperature = kwargs.get("temperature", 1.0)
            if temperature == 0:
                self.cache.set(model, messages, content, **kwargs)

        return {
            "content": content,
            "model": model,
            "from_cache": False,
            "tokens": {
                "input": response.usage.prompt_tokens,
                "output": response.usage.completion_tokens
            }
        }

    def _is_over_budget(self) -> bool:
        """Check if daily budget is exceeded."""
        if self.config.budget_limit_per_day is None:
            return False
        return False  # Implement budget tracking
```

## Best Practices

1. **Caching** - Cache deterministic responses (temperature=0), use content-based keys, set appropriate TTLs
2. **Prompt Engineering** - Keep prompts concise, remove redundancy, use structured outputs
3. **Batching** - Group similar requests, use async for parallelism
4. **Monitoring** - Track cache hit rates, monitor cost savings

## Cost Reduction Checklist

- [ ] Response caching implemented
- [ ] Prompt compression enabled
- [ ] Model routing configured
- [ ] Budget limits set
- [ ] Cost tracking active
- [ ] Cache hit rate monitored
- [ ] Batch processing for bulk tasks

## Common Pitfalls

1. **Over-caching** - Stale responses for dynamic content
2. **Aggressive Compression** - Losing important context
3. **Cache Without TTL** - Outdated responses
4. **Premature Optimization** - Sacrificing quality for cost


## Agent Selection

| Task | Model | Rationale |
|------|-------|----------|
| Cost analysis | sonnet | Financial analysis |
| Optimization opportunities | sonnet | Strategy development |
| Implementation planning | sonnet | Action planning |

## Sources

- [OpenAI Batch API](https://platform.openai.com/docs/guides/batch)
- [tiktoken Library](https://github.com/openai/tiktoken)
- [Redis Documentation](https://redis.io/docs)
- [OpenAI Caching Guide](https://platform.openai.com/docs/guides/prompt-caching)
- [Cost Optimization Best Practices](https://platform.openai.com/docs/guides/production-best-practices/reducing-costs)
