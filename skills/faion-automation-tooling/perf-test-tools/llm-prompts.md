# Performance Testing Tools - LLM-Assisted Prompts

Use these prompts with Claude, ChatGPT, or other LLMs to accelerate development.

## Setup & Configuration Prompts

### Prompt 1: Initial Setup

```
I need to set up performance testing tools for my project.

Project context:
- Language/framework: [YOUR FRAMEWORK]
- Current setup: [DESCRIBE YOUR SETUP]
- Goal: [WHAT YOU WANT TO ACHIEVE]

Please provide:
1. Step-by-step setup instructions
2. Configuration file examples
3. Best practices for my use case
```

### Prompt 2: Configuration Review

```
Review my performance testing tools configuration and suggest improvements.

Current configuration:
[PASTE YOUR CONFIG]

Constraints:
- [LIST ANY CONSTRAINTS]

Please identify:
1. Potential issues
2. Security concerns
3. Performance optimizations
4. Missing best practices
```

## Implementation Prompts

### Prompt 3: Generate Implementation

```
Generate a performance testing tools implementation with these requirements:

Requirements:
- [REQUIREMENT 1]
- [REQUIREMENT 2]
- [REQUIREMENT 3]

Technical constraints:
- Must integrate with: [EXISTING SYSTEMS]
- Performance target: [TARGET]
- Scale: [EXPECTED LOAD]

Include:
1. Full implementation code
2. Error handling
3. Tests
4. Documentation
```

### Prompt 4: Refactor Existing Code

```
Refactor this code to follow performance testing tools best practices.

Current code:
[PASTE YOUR CODE]

Focus on:
- k6, Locust, JMeter
- Maintainability
- Performance
- Error handling

Explain what you changed and why.
```

## Testing Prompts

### Prompt 5: Generate Tests

```
Generate comprehensive tests for this performance testing tools implementation.

Code to test:
[PASTE YOUR CODE]

Requirements:
- Unit tests for all functions
- Integration tests for main flows
- Edge cases and error conditions
- Test coverage > 80%

Use pytest/unittest/jest (specify framework).
```

### Prompt 6: Test Strategy

```
Create a testing strategy for performance testing tools.

Project scope:
- [DESCRIBE PROJECT]
- [KEY FEATURES]

Provide:
1. Test levels (unit, integration, E2E)
2. What to test at each level
3. Testing tools and frameworks
4. CI/CD integration approach
```

## Debugging Prompts

### Prompt 7: Debug Issue

```
I'm having an issue with my performance testing tools implementation.

Error/issue:
[DESCRIBE THE PROBLEM]

Code:
[PASTE RELEVANT CODE]

What I've tried:
- [ATTEMPT 1]
- [ATTEMPT 2]

Help me:
1. Identify the root cause
2. Suggest a fix
3. Prevent this in the future
```

### Prompt 8: Performance Analysis

```
Analyze the performance of my performance testing tools implementation.

Code:
[PASTE CODE]

Current metrics:
- [METRIC 1]: [VALUE]
- [METRIC 2]: [VALUE]

Performance targets:
- [TARGET 1]
- [TARGET 2]

Suggest optimizations with code examples.
```

## Documentation Prompts

### Prompt 9: Generate Documentation

```
Generate comprehensive documentation for this performance testing tools implementation.

Code:
[PASTE CODE]

Include:
1. Overview and purpose
2. Installation/setup
3. API reference
4. Usage examples
5. Configuration options
6. Troubleshooting guide

Format: Markdown
```

### Prompt 10: Explain to Team

```
Explain performance testing tools to my team who are unfamiliar with it.

Team background:
- [DESCRIBE TEAM EXPERIENCE]
- [CURRENT PRACTICES]

Create:
1. Simple explanation (what & why)
2. Benefits for our use case
3. Getting started guide
4. Common pitfalls to avoid
5. Learning resources

Keep it practical and example-driven.
```

## Migration Prompts

### Prompt 11: Migration Plan

```
Create a migration plan to adopt performance testing tools.

Current state:
- [DESCRIBE CURRENT IMPLEMENTATION]
- [PAIN POINTS]

Target state:
- [WHAT YOU WANT TO ACHIEVE]

Provide:
1. Step-by-step migration plan
2. Risk assessment
3. Rollback strategy
4. Timeline estimate
5. Success metrics
```

### Prompt 12: Gradual Adoption

```
Design a gradual adoption strategy for performance testing tools.

Context:
- Team size: [NUMBER]
- Current workload: [BUSY/MODERATE/LIGHT]
- Risk tolerance: [HIGH/MEDIUM/LOW]

Create a phased approach:
1. Phase 1: [PILOT]
2. Phase 2: [EXPAND]
3. Phase 3: [FULL ADOPTION]

Include success criteria for each phase.
```

## Optimization Prompts

### Prompt 13: Best Practices Audit

```
Audit my performance testing tools implementation against best practices.

Code:
[PASTE CODE]

Check for:
- k6, Locust, JMeter
- Security issues
- Performance bottlenecks
- Maintainability concerns
- Missing features

Provide prioritized recommendations.
```

### Prompt 14: Production Readiness

```
Assess production readiness of my performance testing tools implementation.

Code/config:
[PASTE CODE OR CONFIG]

Check:
- Error handling
- Logging/monitoring
- Security
- Performance under load
- Scalability
- Documentation

List blockers, warnings, and nice-to-haves.
```

## Learning Prompts

### Prompt 15: Deep Dive Tutorial

```
Create a tutorial teaching performance testing tools from basics to advanced.

My background:
- [YOUR EXPERIENCE LEVEL]
- [FAMILIAR TECHNOLOGIES]

Cover:
1. Fundamentals
2. Core concepts
3. Practical examples
4. Advanced techniques
5. Real-world scenarios

Make it hands-on with code examples.
```

## Quick Reference Prompts

### One-Liner Prompts

```
# Quick help
"Explain performance testing tools in simple terms"

# Quick example
"Show me a minimal performance testing tools example"

# Quick fix
"Fix this performance testing tools error: [ERROR]"

# Quick comparison
"Compare performance testing tools with [ALTERNATIVE]"

# Quick recommendation
"Should I use performance testing tools for [USE CASE]?"
```
